input {
  beats { 
    port => 9400
  }
}

filter {
  if [message] =~ /"type":"throughput"/ {
    # Sanitise diags, preserving any trailing comma
    mutate {
        gsub => [
            "message",
            '(?m)"diags"\s*:\s*"(?:\\.|[^"\\])*?"\s*,', '"diags":"[removed_for_parsing]",',
            "message",
            '(?m)"diags"\s*:\s*"(?:\\.|[^"\\])*?"',      '"diags":"[removed_for_parsing]"'
        ]
    }
  }
}

#
# connectivity_filters.conf - updated DHCP & 802.11 parsing and outputs
#

#######################################################################
# FILTER: connectivity (pssid-dhcp / pssid-80211)                     #
#######################################################################
filter {
  #
  # Match any line that contains pssid-dhcp or pssid-80211 (incl. teardown)
  #
  if [message] =~ /pssid-(?:dhcp|80211)/ {
    
    # Keep a copy of the full raw message for troubleshooting
    mutate {
      add_field => { "[@metadata][original_message]" => "%{message}" }
    }

    grok {
      match => {
        "message" => "%{GREEDYDATA:syslog_prefix}pssid:\s*pssid-(?<connectivity_test_type>dhcp|80211)(?:\s+(?<connectivity_action>\w+))?\s*:\s*(?<connectivity_json>\{.*)$"
      }
      tag_on_failure => ["_grokparsefailure_connectivity"]
    }

    #
    # Parse the embedded JSON block
    #
    if "_grokparsefailure_connectivity" not in [tags] {
      json {
        source => "connectivity_json"
        target => "connectivity_data"
        tag_on_failure => ["_jsonparsefailure_connectivity"]
      }
    }

    #
    # Build normalised fields only if JSON was parsed OK
    #
    if "_jsonparsefailure_connectivity" not in [tags] {

      mutate {
        add_field => {
          "[connectivity][test_type]"    => "%{connectivity_test_type}"
          "[connectivity][action]"       => "%{connectivity_action}"
          "[connectivity][status_code]"  => "%{[connectivity_data][status_code]}"
          "[connectivity][status_msg]"   => "%{[connectivity_data][status_msg]}"
        }
      }

      #
      # Common helper to generate safe SSID
      #
      ruby {
        code => '
          ssid = event.get("[connectivity_data][ssid]") ||
                 event.get("[connectivity][wifi_ssid]")
          if ssid
            event.set("[connectivity][wifi_ssid]", ssid)
            safe = ssid.downcase.gsub(/[^a-z0-9]+/, "_")
            event.set("[connectivity][wifi_ssid_safe]", safe)
          else
            event.set("[connectivity][wifi_ssid_safe]", "unknown")
          end
        '
      }

      #
      # DHCP ONLY -----------------------------------------------------
      #
      if [connectivity_test_type] == "dhcp" {
        ruby {
          code => '
            log = event.get("[connectivity_data][dhcpcd_log]") || []
            start = nil; lease = nil
            log.each do |e|
              m = e["msg"] || ""
              t = e["time"]
              start = t if m.include?("soliciting a DHCP lease") && start.nil?
              if (m =~ /leased (.+) for/)
                lease = t
                event.set("[connectivity][assigned_ip]", $1)
              end
            end
            if start && lease
              event.set("[connectivity][dhcp_lease_duration_seconds]", lease - start)
            end
            event.set("[connectivity][dhcp_event_count]", log.length)
          '
        }
      }

      #
      # 802.11 ONLY ---------------------------------------------------
      #
      if [connectivity_test_type] == "80211" {
        ruby {
          code => '
            wpa = event.get("[connectivity_data][wpa_log]") || []
            assoc = nil; conn = nil; ssid = nil; bssid = nil
            wpa.each do |e|
              m = e["msg"] || ""
              t = e["time"]
              if m =~ /Trying to associate with SSID \'([^\']+)\'/ && assoc.nil?
                assoc = t; ssid = $1
              end
              if m =~ /CTRL-EVENT-CONNECTED.*Connection to ([A-Fa-f0-9:]{17})/
                conn = t; bssid = $1
              end
            end
            if assoc && conn
              event.set("[connectivity][wifi_association_duration_seconds]", conn - assoc)
            end
            if ssid
              event.set("[connectivity][wifi_ssid]", ssid)
              safe = ssid.downcase.gsub(/[^a-z0-9]+/, "_")
              event.set("[connectivity][wifi_ssid_safe]", safe)
            end
            event.set("[connectivity][wifi_bssid]", bssid) if bssid
            event.set("[connectivity][wifi_event_count]", wpa.length)
            event.set("[connectivity][eap_auth_success]", wpa.any?{|e| (e["msg"]||"").include?("EAP authentication completed successfully") })
            event.set("[connectivity][disconnected]", wpa.any?{|e| (e["msg"]||"").include?("CTRL-EVENT-DISCONNECTED") })
          '
        }
      }

      #
      # Tag and clean up
      #
      mutate {
        add_tag => ["connectivity_test", "pssid_%{[connectivity][test_type]}"]
        remove_field => ["connectivity_data", "connectivity_json", "connectivity_test_type", "connectivity_action", "syslog_prefix"]
        # Prevent pscheduler filters from touching connectivity events
        add_tag => ["skip_pscheduler_processing"]
      }
    }
    if ![connectivity][wifi_ssid_safe] {
            mutate { add_field => { "[connectivity][wifi_ssid_safe]" => "unknown" } }
    }
    if [connectivity_test_type] == "80211" and ![connectivity][wifi_association_duration_seconds] {
        ruby {
            code => "event.set('[connectivity][wifi_association_duration_seconds]', nil)"
        }
    }

    if [connectivity_test_type] == "dhcp" and ![connectivity][dhcp_lease_duration_seconds] {
        ruby {
            code => "event.set('[connectivity][dhcp_lease_duration_seconds]', nil)"
        }
    }
  }
}

#######################################################################
# OUTPUT: connectivity                                                #
#######################################################################




# from https://github.com/perfsonar/logstash/blob/master/perfsonar-logstash/perfsonar-logstash/pipeline/02-pscheduler_common.conf
# Do some common tasks common to all tasks:
#   1. Remove the schedule object so we can replace with [task][schedule]
#   2. Build the pscheduler object
#   3. Remove run, task and tool
#   4. Generate a checksum used to identify same test with ruby script
#           NOTE: fingerprint filter can't handle nested objects

filter {
    # Skip processing for connectivity tests
    if "skip_pscheduler_processing" not in [tags] {
        if [message] {
            # First, check if this is a pssid message and parse accordingly
            if [message] =~ "^.*pssid:\\s*\\{" {
                # Extract just the JSON part from pssid messages
                grok {
                    match => { "message" => "pssid:\s*(?<json_message>\{.+\}$)" }
                    tag_on_failure => ["_grokparsefailure_pssid"]
                }
                
                if "_grokparsefailure_pssid" not in [tags] {
                    mutate {
                        replace => { "message" => "%{json_message}" }
                        remove_field => ["json_message"]
                    }
                    
                    # Parse the JSON
                    json {
                        source => "message"
                        tag_on_failure => ["_jsonparsefailure_pscheduler"]
                    }
                }
            } else {
                # For non-pssid messages, try the old format
                grok {
                    match => { "message" => "pssid: %{GREEDYDATA:json_message}" }
                }

                mutate {
                    replace => { "message" => "%{json_message}" }
                    remove_field => ["json_message"]
                }
            }

            # Only run Ruby script if we don't already have parsed JSON
            if "_jsonparsefailure_pscheduler" not in [tags] and ![test] {
                ruby {
                    path => "/usr/share/logstash/pipeline/ruby/pscheduler_proxy_normalize.rb"
                }
                if [pscheduler_event] {
                    ruby {
                        code => '
                            event.get("pscheduler_event").each { |k, v| event.set(k, v) }
                        '
                    }
                    mutate {
                        remove_field => ["pscheduler_event"]
                    }
                }
            }

        }

        mutate {
            remove_field => [ "schedule" ]
        }
        
        mutate {
            rename => {
                "[task][schedule]" => "[schedule]"
                "[run][added]" => "[pscheduler][added]"
                "[run][start-time]" => "[pscheduler][start_time]"
                "[run][end-time]" => "[pscheduler][end_time]"
                "[participants]" => "[pscheduler][participants]"
                "[tool][name]" => "[pscheduler][tool]"
                "[task][detail][duration]" => "[pscheduler][duration]"
                "[task][href]" => "[pscheduler][task_href]"
                "[run][href]" => "[pscheduler][run_href]"
            }
            remove_field => ["run", "task", "tool"]
        }
        
        if [pscheduler][task_href] and [pscheduler][run_href] {
          dissect {
              mapping => {
                  "[pscheduler][task_href]" => "%{?url}/tasks/%{[pscheduler][task_id]}"
                  "[pscheduler][run_href]" => "%{?url}/runs/%{[pscheduler][run_id]}"
              }
          }
        }

        #if using HTTP connector, have the option to use custom header that gives better hint of observer
        mutate {
            rename => {
                "[headers][x_ps_observer]" => "[@metadata][ps_observer]"
            }
        }
        #remove HTTP header fields if present
        mutate {
            remove_field => ["headers"]
        }
        
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_test_checksum.rb"
        }
    }
}

# Make source, dest and ip_version consistent
filter {
    if "skip_pscheduler_processing" not in [tags] {
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_normalize_endpoints.rb"
        }
    }
}

# Convert IS8601 durations in common fields to seconds
filter {
    if "skip_pscheduler_processing" not in [tags] {
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_iso8601_duration.rb"
            script_params => { 
                "fields" => [
                    "[pscheduler][duration]",
                    "[schedule][slip]",
                    "[schedule][repeat]"
                ]
            }
        }
    }
}

# Lookup GeoIP information for IP fields
filter {
    if "skip_pscheduler_processing" not in [tags] {
        if [meta][source][ip] {
            geoip {
                default_database_type => "City"
                source => "[meta][source][ip]"
                target => "[meta][source][geo]"
                fields => [ "location", "city_name", "country_name", "continent_name" ]
            }
            geoip {
              default_database_type => "ASN"
              fields => [ "autonomous_system_number", "autonomous_system_organization" ]
              source => "[meta][source][ip]"
              target => "[meta][source][geo][as]"
            }
            mutate {
              rename => {
                "[meta][source][geo][as][asn]"    => "[meta][source][geo][as][number]"
                "[meta][source][geo][as][as_org]" => "[meta][source][as][organization]"
              }
            }
        }
        
        if [meta][destination][ip] {
            geoip {
                default_database_type => "City"
                source => "[meta][destination][ip]"
                target => "[meta][destination][geo]"
                fields => [ "location", "city_name", "country_name", "continent_name" ]
            }
            geoip {
              default_database_type => "ASN"
              fields => [ "autonomous_system_number", "autonomous_system_organization" ]
              source => "[meta][destination][ip]"
              target => "[meta][destination][geo][as]"
            }
            mutate {
              rename => {
                "[meta][destination][geo][as][asn]"    => "[meta][destination][geo][as][number]"
                "[meta][destination][geo][as][as_org]" => "[meta][destination][as][organization]"
              }
            }
        }
        
        if [meta][observer][ip] {
            geoip {
                default_database_type => "City"
                source => "[meta][observer][ip]"
                target => "[meta][observer][geo]"
                fields => [ "location", "city_name", "country_name", "continent_name" ]
            }
            geoip {
              default_database_type => "ASN"
              fields => [ "autonomous_system_number", "autonomous_system_organization" ]
              source => "[meta][observer][ip]"
              target => "[meta][observer][geo][as]"
            }
            mutate {
              rename => {
                "[meta][observer][geo][as][asn]"    => "[meta][observer][geo][as][number]"
                "[meta][observer][geo][as][as_org]" => "[meta][observer][as][organization]"
              }
            }
        }
    }
}


# HTTP TEST
filter {
    if [test][type] == "http" and "skip_pscheduler_processing" not in [tags] {
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_iso8601_duration.rb"
            script_params => { 
                "fields" => [
                    "[result][time]"
                ]
            }
        }
        mutate {
            lowercase => ["[reference][SSID]"]
        }
    }
}

###########################################################################
# 30-pscheduler_throughput.conf
# Sanitises and parses pscheduler “throughput” events
###########################################################################

filter {
  if [test][type] == "throughput" {

    # ruby {
    #   path => "/usr/share/logstash/pipeline/ruby/pscheduler_iso8601_duration.rb"
    #   script_params => { "fields" => ["[pscheduler][duration]"] }
    # }

    mutate {
      rename => {
        "[result][succeeded]"                                => "[@metadata][result][succeeded]"
        "[result][summary][summary][throughput-bits]"        => "[@metadata][result][throughput]"
        "[result][summary][streams][rtt]"                    => "[@metadata][result][rtt]"
        "[result][summary][streams]"                         => "[@metadata][result][streams][json]"
        "[result][summary][summary][retransmits]"            => "[@metadata][result][retransmits]"
      }
    }

    ## ***DROP*** the huge per-second array
    mutate { remove_field => ["[result][intervals]"] }

    ## throw away the now-empty result container
    mutate { remove_field => ["result"] }

    ## bring the good stuff back into the event body
    mutate { rename => { "[@metadata][result]" => "result" } }

    mutate { lowercase => ["[reference][SSID]"] }

    ruby {
        code => '
            participants = event.get("[pscheduler][participants]")
            if participants.is_a?(Array)
            event.set("[pscheduler][participants_joined]", participants.join("_"))
            end
        '
    }
  }
}


filter {
    if [test][type] == "latency" or [test][type] == "latencybg" and "skip_pscheduler_processing" not in [tags] {
        mutate {
            rename => {
                "[result][succeeded]" => "[@metadata][result][succeeded]"
                "[result][error]" => "[@metadata][result][error]"
                "[result][max-clock-error]" => "[@metadata][result][max_clock_error]"
                "[result][packets-duplicated]" => "[@metadata][result][packets][duplicated]"
                "[result][packets-lost]" => "[@metadata][result][packets][lost]"
                "[result][packets-received]" => "[@metadata][result][packets][received]"
                "[result][packets-reordered]" => "[@metadata][result][packets][reordered]"
                "[result][packets-sent]" => "[@metadata][result][packets][sent]"
            }
        }
        
        #calculate packet loss
        ruby {
            code => "
                sent = event.get('[@metadata][result][packets][sent]')
                lost = event.get('[@metadata][result][packets][lost]')
                if lost and sent and sent > 0 then
                    event.set('[@metadata][result][packets][loss]', lost.to_f/sent.to_f)
                end
            "
        }

        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_histogram.rb"
            script_params => {
                "source" => "[result][histogram-latency]"
                "target" => "[@metadata][result][latency]"
                #you can also configure quantiles
            }
        }
        
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_histogram.rb"
            script_params => {
                "source" => "[result][histogram-ttl]"
                "target" => "[@metadata][result][ttl]"
            }
        }
        
        mutate {
            remove_field => ["result"]
        }

        mutate {
            lowercase => ["[reference][SSID]"]
        }
        
        mutate {
            rename => { 
                "[@metadata][result]" => "result"
            }
        }
        
    }
}

filter {
    if [test][type] == "rtt" and "skip_pscheduler_processing" not in [tags] {
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_iso8601_duration.rb"
            script_params => { 
                "fields" => [
                    "[test][spec][interval]",
                    "[test][spec][timeout]",
                    "[test][spec][deadline]",
                    "[result][min]",
                    "[result][max]",
                    "[result][mean]",
                    "[result][stddev]"
                ]
            }
        }
        
        mutate {
            rename => {
                "[result][succeeded]" => "[@metadata][result][succeeded]"
                "[result][error]" => "[@metadata][result][error]"
                "[result][duplicates]" => "[@metadata][result][packets][duplicated]"
                "[result][lost]" => "[@metadata][result][packets][lost]"
                "[result][loss]" => "[@metadata][result][packets][loss]"
                "[result][received]" => "[@metadata][result][packets][received]"
                "[result][reorders]" => "[@metadata][result][packets][reordered]"
                "[result][sent]" => "[@metadata][result][packets][sent]"
                "[result][roundtrips]" => "[@metadata][result][packets][json]"
                "[result][min]" => "[@metadata][result][rtt][min]"
                "[result][max]" => "[@metadata][result][rtt][max]"
                "[result][mean]" => "[@metadata][result][rtt][mean]"
                "[result][stddev]" => "[@metadata][result][rtt][stddev]"
            }
        }
        
        mutate {
            remove_field => ["result"]
        }

        mutate {
            lowercase => ["[reference][SSID]"]
        }

        mutate {
            rename => { 
                "[@metadata][result]" => "result"
            }
        }
       
    }
}

# https://github.com/perfsonar/logstash/blob/master/perfsonar-logstash/perfsonar-logstash/pipeline/99-outputs.conf
output {
    
    if [test][type] == "throughput" {
        opensearch {
            hosts => ["${OPENSEARCH_HOST}"]
            ssl_certificate_verification => false
            user => "${OPENSEARCH_USER}"
            password => "${OPENSEARCH_PASSWORD}"
            action => "create"
            index => "pscheduler_%{[test][type]}_%{[agent][name]}_%{[reference][SSID]}_%{[pscheduler][participants_joined]}"
        }
    } else if [test][type] == "rtt" and [result][rtt] {
        opensearch {
            hosts => ["${OPENSEARCH_HOST}"]
            ssl_certificate_verification => false
            user => "${OPENSEARCH_USER}"
            password => "${OPENSEARCH_PASSWORD}"
            action => "create"
            index => "pscheduler_%{[test][type]}_%{[agent][name]}_%{[reference][SSID]}_%{[test][spec][dest]}"
        }
    } else if [test][type] == "latency" {
        opensearch {
            hosts => ["${OPENSEARCH_HOST}"]
            ssl_certificate_verification => false
            user => "${OPENSEARCH_USER}"
            password => "${OPENSEARCH_PASSWORD}"
            action => "create"
            index => "pscheduler_%{[test][type]}_%{[agent][name]}_%{[reference][SSID]}_%{[test][spec][dest]}"
        }
    } else if [test][type] and [agent][name] and [reference][SSID] {
        opensearch {
            hosts => ["${OPENSEARCH_HOST}"]
            ssl_certificate_verification => false
            user => "${OPENSEARCH_USER}"
            password => "${OPENSEARCH_PASSWORD}"
            action => "create"
            index => "pscheduler_%{[test][type]}_%{[agent][name]}_%{[reference][SSID]}"
        }
    } else if [test][type] {
        # Fallback for tests missing agent name or SSID
        stdout {
            codec => line {
                format => "MISSING FIELDS: test_type=%{[test][type]} agent=%{[agent][name]} ssid=%{[reference][SSID]}"
            }
        }
    }
    


    if [connectivity][test_type] and [agent][name] {
        opensearch {
            hosts => ["${OPENSEARCH_HOST}"]
            ssl_certificate_verification => false
            user => "${OPENSEARCH_USER}"
            password => "${OPENSEARCH_PASSWORD}"
            action => "create"
            index => "pscheduler_%{[connectivity][test_type]}_%{[agent][name]}_%{[connectivity][wifi_ssid_safe]}"
        }
  }
}


output {
  stdout {
    codec => rubydebug
  }
}