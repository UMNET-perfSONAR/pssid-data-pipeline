input {
  beats { 
    port => 9400
  }
}

# from https://github.com/perfsonar/logstash/blob/master/perfsonar-logstash/perfsonar-logstash/pipeline/02-pscheduler_common.conf
# Do some common tasks common to all tasks:
#   1. Remove the schedule object so we can replace with [task][schedule]
#   2. Build the pscheduler object
#   3. Remove run, task and tool
#   4. Generate a checksum used to identify same test with ruby script
#           NOTE: fingerprint filter can't handle nested objects

filter {
    if [message] {
        grok {
          match => { "message" => "pssid: %{GREEDYDATA:json_message}" }
        }

        mutate {
          replace => { "message" => "%{json_message}" }
          remove_field => ["json_message"]
        }

        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_proxy_normalize.rb"
        }
        if [pscheduler_event] {
            ruby {
                code => '
                    event.get("pscheduler_event").each { |k, v| event.set(k, v) }
                '
            }
            mutate {
                remove_field => ["pscheduler_event"]
            }
        }

        mutate {
            add_field => { 
                "debug_original_message_length" => "%{[message][length]}"
                "debug_original_message_ends_with" => "%{[message][-50..-1]}"
            }
        }
    }

    mutate {
        remove_field => [ "schedule" ]
    }
    
    mutate {
        rename => {
            "[task][schedule]" => "[schedule]"
            "[run][added]" => "[pscheduler][added]"
            "[run][start-time]" => "[pscheduler][start_time]"
            "[run][end-time]" => "[pscheduler][end_time]"
            "[participants]" => "[pscheduler][participants]"
            "[tool][name]" => "[pscheduler][tool]"
            "[task][detail][duration]" => "[pscheduler][duration]"
            "[task][href]" => "[pscheduler][task_href]"
            "[run][href]" => "[pscheduler][run_href]"
        }
        remove_field => ["run", "task", "tool"]
    }
    
    if [pscheduler][task_href] and [pscheduler][run_href] {
      dissect {
          mapping => {
              "[pscheduler][task_href]" => "%{?url}/tasks/%{[pscheduler][task_id]}"
              "[pscheduler][run_href]" => "%{?url}/runs/%{[pscheduler][run_id]}"
          }
      }
    }

    #if using HTTP connector, have the option to use custom header that gives better hint of observer
    mutate {
        rename => {
            "[headers][x_ps_observer]" => "[@metadata][ps_observer]"
        }
    }
    #remove HTTP header fields if present
    mutate {
        remove_field => ["headers"]
    }
    
    ruby {
        path => "/usr/share/logstash/pipeline/ruby/pscheduler_test_checksum.rb"
    }
}

# Make source, dest and ip_version consistent

filter {
    ruby {
        path => "/usr/share/logstash/pipeline/ruby/pscheduler_normalize_endpoints.rb"
    }
}

# Convert IS8601 durations in common fields to seconds
filter {
    ruby {
        path => "/usr/share/logstash/pipeline/ruby/pscheduler_iso8601_duration.rb"
        script_params => { 
            "fields" => [
                "[pscheduler][duration]",
                "[schedule][slip]",
                "[schedule][repeat]"
            ]
        }
    }
}

# Lookup GeoIP information for IP fields
filter {
    if [meta][source][ip] {
        geoip {
            default_database_type => "City"
            source => "[meta][source][ip]"
            target => "[meta][source][geo]"
            fields => [ "location", "city_name", "country_name", "continent_name" ]
        }
        geoip {
          default_database_type => "ASN"
          fields => [ "autonomous_system_number", "autonomous_system_organization" ]
          source => "[meta][source][ip]"
          target => "[meta][source][geo][as]"
        }
        mutate {
          rename => {
            "[meta][source][geo][as][asn]"    => "[meta][source][geo][as][number]"
            "[meta][source][geo][as][as_org]" => "[meta][source][as][organization]"
          }
        }
    }
    
    if [meta][destination][ip] {
        geoip {
            default_database_type => "City"
            source => "[meta][destination][ip]"
            target => "[meta][destination][geo]"
            fields => [ "location", "city_name", "country_name", "continent_name" ]
        }
        geoip {
          default_database_type => "ASN"
          fields => [ "autonomous_system_number", "autonomous_system_organization" ]
          source => "[meta][destination][ip]"
          target => "[meta][destination][geo][as]"
        }
        mutate {
          rename => {
            "[meta][destination][geo][as][asn]"    => "[meta][destination][geo][as][number]"
            "[meta][destination][geo][as][as_org]" => "[meta][destination][as][organization]"
          }
        }
    }
    
    if [meta][observer][ip] {
        geoip {
            default_database_type => "City"
            source => "[meta][observer][ip]"
            target => "[meta][observer][geo]"
            fields => [ "location", "city_name", "country_name", "continent_name" ]
        }
        geoip {
          default_database_type => "ASN"
          fields => [ "autonomous_system_number", "autonomous_system_organization" ]
          source => "[meta][observer][ip]"
          target => "[meta][observer][geo][as]"
        }
        mutate {
          rename => {
            "[meta][observer][geo][as][asn]"    => "[meta][observer][geo][as][number]"
            "[meta][observer][geo][as][as_org]" => "[meta][observer][as][organization]"
          }
        }
    }
    
}

 # HTTP TEST
 filter {
    if [test][type] == "http"{
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_iso8601_duration.rb"
            script_params => { 
                "fields" => [
                    "[result][time]"
                ]
            }
        }
    }
}

# throughput test?
filter {
    if [test][type] == "throughput" {
        
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_diags_json_parse.rb"
        }

        # Extract available result fields
        mutate {
            rename => {
                "[result-full-parsed][sum_received][bits_per_second]" => "[@metadata][result][bits_per_second_received]"
                "[result-full-parsed][sum_received][bytes]"           => "[@metadata][result][bytes][received]"
                "[result-full-parsed][sum_sent][bits_per_second]"     => "[@metadata][result][bits_per_second_sent]"
                "[result-full-parsed][sum_sent][bytes]"             => "[@metadata][result][bytes][sent]"
            }
        }

        # Clean up raw fields
        mutate {
            rename => {
                "[@metadata][result]" => "result"
            }
        }
    }
}


filter {
  

  # Parse the JSON payload
  json {
    source => "json_payload"
    target => "pssid_data"
  }

  # Promote key fields from JSON
  if [pssid_data][status_code] {
    mutate {
      add_field => {
        "status_code" => "%{[pssid_data][status_code]}"
        "status_msg"  => "%{[pssid_data][status_msg]}"
        "pssid_subtype" => "%{pssid_subtype}"
      }
    }
  }

  # Optional: Expand pssid_log array into individual events
  if [pssid_data][pssid_log] {
    split {
      field => "[pssid_data][pssid_log]"
    }
  }

  # Optional: Expand dhcpcd_log array into individual events
  if [pssid_data][dhcpcd_log] {
    split {
      field => "[pssid_data][dhcpcd_log]"
    }
  }

  # Optional: Expand wpa_log array into individual events
  if [pssid_data][wpa_log] {
    split {
      field => "[pssid_data][wpa_log]"
    }
  }

  # Optional: You can flatten deeply nested fields if needed
}




filter {
    if [test][type] == "latency" or [test][type] == "latencybg"{
        mutate {
            rename => {
                "[result][succeeded]" => "[@metadata][result][succeeded]"
                "[result][error]" => "[@metadata][result][error]"
                "[result][max-clock-error]" => "[@metadata][result][max_clock_error]"
                "[result][packets-duplicated]" => "[@metadata][result][packets][duplicated]"
                "[result][packets-lost]" => "[@metadata][result][packets][lost]"
                "[result][packets-received]" => "[@metadata][result][packets][received]"
                "[result][packets-reordered]" => "[@metadata][result][packets][reordered]"
                "[result][packets-sent]" => "[@metadata][result][packets][sent]"
            }
        }
        
        #calculate packet loss
        ruby {
            code => "
                sent = event.get('[@metadata][result][packets][sent]')
                lost = event.get('[@metadata][result][packets][lost]')
                if lost and sent and sent > 0 then
                    event.set('[@metadata][result][packets][loss]', lost.to_f/sent.to_f)
                end
            "
        }

        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_histogram.rb"
            script_params => {
                "source" => "[result][histogram-latency]"
                "target" => "[@metadata][result][latency]"
                #you can also configure quantiles
            }
        }
        
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_histogram.rb"
            script_params => {
                "source" => "[result][histogram-ttl]"
                "target" => "[@metadata][result][ttl]"
            }
        }
        
        mutate {
            remove_field => ["result"]
        }
        
        mutate {
            rename => { 
                "[@metadata][result]" => "result"
            }
        }
    }
}

filter {
    if [test][type] == "rtt"{
        
        ruby {
            path => "/usr/share/logstash/pipeline/ruby/pscheduler_iso8601_duration.rb"
            script_params => { 
                "fields" => [
                    "[test][spec][interval]",
                    "[test][spec][timeout]",
                    "[test][spec][deadline]",
                    "[result][min]",
                    "[result][max]",
                    "[result][mean]",
                    "[result][stddev]"
                ]
            }
        }
        
        mutate {
            rename => {
                "[result][succeeded]" => "[@metadata][result][succeeded]"
                "[result][error]" => "[@metadata][result][error]"
                "[result][duplicates]" => "[@metadata][result][packets][duplicated]"
                "[result][lost]" => "[@metadata][result][packets][lost]"
                "[result][loss]" => "[@metadata][result][packets][loss]"
                "[result][received]" => "[@metadata][result][packets][received]"
                "[result][reorders]" => "[@metadata][result][packets][reordered]"
                "[result][sent]" => "[@metadata][result][packets][sent]"
                "[result][roundtrips]" => "[@metadata][result][packets][json]"
                "[result][min]" => "[@metadata][result][rtt][min]"
                "[result][max]" => "[@metadata][result][rtt][max]"
                "[result][mean]" => "[@metadata][result][rtt][mean]"
                "[result][stddev]" => "[@metadata][result][rtt][stddev]"
            }
        }
        
        mutate {
            remove_field => ["result"]
        }
        
        mutate {
            rename => { 
                "[@metadata][result]" => "result"
            }
        }
    }
}

# https://github.com/perfsonar/logstash/blob/master/perfsonar-logstash/perfsonar-logstash/pipeline/99-outputs.conf
output {
    if [test][type] {
        opensearch {
            hosts => ["${OPENSEARCH_HOST}"]
            ssl_certificate_verification => false
            user => "${OPENSEARCH_USER}"
            password => "${OPENSEARCH_PASSWORD}"
            action => "create"
            index => "pscheduler_%{[test][type]}_%{[agent][name]}_%{[reference][SSID]}"
        }
    }
}

output {
  stdout {
    codec => rubydebug
  }
}

output {
  file {
    path => "/tmp/all_logstash_events.log"
    codec => json_lines
  }
}




# might rename index to "pscheduler_%{[test][type]}_%{[agent][name]}_%{+YYYY.MM.dd}"
